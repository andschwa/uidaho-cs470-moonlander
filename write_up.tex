\documentclass[12pt, article]{scrartcl}
\usepackage[english]{babel}
\usepackage{sectsty}
\allsectionsfont{\centering \normalfont\scshape}
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}
\fancyfoot[L]{}
\fancyfoot[C]{}
\fancyfoot[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\setlength{\headheight}{13.6pt}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}

\title{	
\normalfont \normalsize 
\textsc{University of Idaho: CS 470 - Artificial Intelligence} \\ [25pt]
\horrule{0.5pt} \\[0.4cm]
\huge Project 3a: Moonlander\\
\horrule{2pt} \\[0.5cm]
}
\author{Andrew Schwartzmeyer}
\date{\normalsize\today}

\begin{document}
\maketitle 
\begin{abstract}
For this project I created a program that utilized an artificial neural network to control and safely land a ``moonlander'' in varying environments. The moonlander and its environment is represented as an object, where upon generation the environment's wind and acceleration are randomized, and the moonlander itself is initialized (put in the air, dropping at some speed directly above its target, with full fuel tanks). The main program runs an episodic experiment that uses a dynamically generated artificial neural network to control the moonlander through each episode, an episode being one attempt from initialization to land, proceeds to process the results through its learner, and then repeats itself.

As the moonlander descends, each tick of the program updates its velocity and position (in both the vertical and horizontal direction), and its at this point that the neural network controls the moonlander via its burn and thrust abilities. The moonlander can convert fuel into a change into an immediate change in velocity: burn to positively increase its velocity on the $y$ axis, and thrust to positively increase its velocity on the $x$ axis (where the gravitational acceleration and wind respectively affect these velocities negatively).

The program is written in Python for ease of development, but would need to be ported to faster language for any real use, as it ran into a problem with execution speed.
\end{abstract}
\pagebreak
\section{Introduction}
This project focused on learning about artificial neural networks to find solutions to real-world problems. Artificial neural networks are inspired by a (now out of date) understanding of biological central nervous systems. They are used to model complex mathematical relationships between inputs and outputs (such as with the moonlander). By training a neural network with sets of data and informing it of the relationshps one wishes it to model, the network can create a multi-variable mathematical equation which can be used to predict new relationships.

For instance, the neural network for the moonlander project has seven input variables: height, $x$ position, $y$ velocity, $x$ velocity, wind, acceleration, and fuel. It transforms these inputs into two outputs: burst and thrust. The goal of the mathematical model is to adjust these two outputs to acheieve a successful landing.

\section{Algorithms}
\subsection{Neural Network and Training Algorithm}
The artificial neural network used in this program is based on a reinforcement learning approach, using a neural-fitted Q-learning algorithm. When the experiment begins, a neural network is dynamically created based on the dimensions of the supplied input and possible actions. On each request for output from the neural network, the current state of the moonlander is converted into a supervised data set that is fed through the generated multi-layer feed-forward network, using a hidden layer implementing the sigmoid squashing function. The NFQ learner uses an epsilon-greedy explorer to search for better actions.

The rewards for the reinforcement learning are calculated after each episode, and correspond to whether that episode successfully landed or crashed. If this were implemented in a faster language, or were to be better optimized, this approach would be entirely successfully. However, in its current form as a pure Python program, it cannot practically calculate enough iterations to successfully produce a useful model. 

\subsection{Critical Problem}
One of the main causes for the severe speed issues is that this approach, to work effectively, needs to examine continuously defined actions (which would also necessitate a change in the algorithms used); however, to implement practically, the actions needed to undergo discretization. Unfortunately the original description of the environment (which used floating point values for all inputs and outputs), indicated that it was meant to be continuous. The best practical discrete range of values I could define produced a list of 441 possible actions for each state (the states remained defined continuously). This proved impractical to calculate quickly.

\section{Results}
Because of its failure to learn well-enough to create a decent model, the average success rate of the neural network is 0; it crashes. With the reinforcement learning approach, during training the network essentially needs to ``stumble upon'' a successful landing, from which it could start to base its model. The resources required to calculate the number of iterations for this to happen in the program's current form are impractical. Running for over 18 hours on a quad-core processor clocked at 3.1 GHz (virtualized access at least through VMWare to an Ubuntu Server virtual machine), it was able to complete 62 episodes, all of which have thus far crashed.

With this training, when run 10 times for each acceleration from 1 to 3, it (of course, considering the problems described above) did not land successfully. The program's major weakness being its speed, it is simply impractical to use. Theoretically, if a more complex approach to the reward training system were developed (say, incremental rewards depending on how ``close'' it was to a successful landing) it could be much more efficient. This would require mathematically defining which crashed states are better than others, and without further definement of the moonlander's environment, I was unable to go this route.

\section{Conclusion}
Although this program failed to produce useful results for the moonlander, it was a useful exercise in learning how neural networks operate. If improved upon, this program could be successful, but is currently not. At this point in my education I do not have the mathematical background to improve this program properly; it would best be a task for a graduate thesis. Essentially it would require implementing a continuous-action value-based reinforcement-learning algorithm, or switching to a policy search method (which can handle continuous actions). With the resources currently available to me, this was not possible, but I would be interested in re-examining this prospect in the future.
\end{document}
