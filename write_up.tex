\documentclass[12pt, article]{scrartcl}
\usepackage[english]{babel}
\usepackage{sectsty}
\allsectionsfont{\centering \normalfont\scshape}
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}
\fancyfoot[L]{}
\fancyfoot[C]{}
\fancyfoot[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\setlength{\headheight}{13.6pt}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}

\title{	
\normalfont \normalsize 
\textsc{University of Idaho: CS 470 - Artificial Intelligence} \\ [25pt]
\horrule{0.5pt} \\[0.4cm]
\huge Project 3a: Moonlander\\
\horrule{2pt} \\[0.5cm]
}
\author{Andrew Schwartzmeyer}
\date{\normalsize\today}

\begin{document}
\maketitle 
\begin{abstract}
This project implemented an artificial neural network as a controller for a simulated moonlander. It is written in Python and utilizes the PyBrain neural network library. This is a redo of my prior submission as it never worked properly (an incorrect algorithm was chosen.) Which much research and re-factoring, it now works properly. A three layer neural network (linear input, sigmoidal hidden, linear output) trained with a stochastic hill climbing optimizer successfully lands the moonlander.
\end{abstract}
\pagebreak
\section{Introduction}
Neural networks are an incredibly powerful way to mathematically model non-linear equations using computers. Given a function with a number of available inputs and a desired number of outputs, a network consisting of connected layers of nodes can be used to model non-linear data. The simplest of these networks is a feed-forward topology: each node in each layer is connected to each node in the next layer, with no cyclic (that is, no upstream) connections. Each neuron/node in has a weight associated with each of its inputs, and it is these weights that are changed during training (either manually or using a machine learning approach). The simplest neuron uses a step activation function to determine when to fire its output; if the sum of the incoming data multiplied by their respective weights exceeds a set threshold (also modifiable during training), the neuron fires. Replacing the step-function with a sigmoidal-function that looks like this: $1/(1+e^(-a/p))$, where $a$ is the input activation, and where $p$ controls the shape of the curve (and will be set to $1$), allows us to curve the activation. Instead of a linear threshold to fire or not to fire, a sigmoidal function applied to the inputs provides a continuously graded output between 0 and 1. This is commonly used for hidden layers, that is, layers that are neither input nor output, but provide weights to be modified by a trainer. This particular project implemented a feed-forward network with one hidden layer.
\section{Algorithms}
\subsection{Neural network structure}
The neural network used by the control is composed of three layers: a linear input layer with 7 nodes; a hidden sigmoidal layer with 5 nodes; and a linear output layer with 2 nodes. The input layer represents data gathered by sensors on the moonlander: height, vertical velocity, vertical acceleration due to gravity, horizontal position, horizontal velocity, horizontal wind, and fuel. When initialized, the moonlander is at height 100, with an initial downward velocity between 0 and 10, a gravitational acceleration between 1 and 3, a horizontal position at 0 (center), no horizontal velocity, a horizontal wind between -0.1 and 0.1, and 100 fuel. All of these are unit-less and so are 1-to-1. The output layer represents burn and thrust. Burn allows the vertical velocity to be decreased by the amount of fuel burned, and thrust allows the horizontal velocity to be adjusted in either direction, also burning fuel. One simulation consists of initializing the moonlander, and then updating the environment on each tick based on the actions (amount to burn and/or thrust) provided by the neural network's output. The network's sigmoidal hidden layer provides us with more adjustable weights and biases (a bias essentially existing as a weight on that node's output). As mentioned earlier, instead of using a step function, the nodes in the hidden layer calculate a graded output based on their received input run through the sigmoidal function shown above, where $p=1$.
\subsubsection{Scaling}
For this project, the input data provided by the sensors is individually normalized to a value between -1 and 1 before being sent to the network. The values used to scale the input are ranges exaggerating the expected input (i.e., with height being between 0 and 100, the scale is something like -50 to 150, so that the extremes are avoided). As the scales cannot be learned, I tried many different ranges, and the final ones used are in the results section. The output of the neural network should not be scaled. I tried denormalizing it for quite a bit, and realized this is, in fact, a bad idea. Not only did it not work, it does not need to be implemented. The unscaled linear output of the neural network after training should fall into the desired range for burn and thrust naturally, because the network, with properly adjusted weights and biases, should output the needed values based on the normalized input.
\subsection{Training}
Although I am personally a fan of the genetic algorithm, its implementation was difficult, and was scrapped in favor of a stochastic hill-climber, which performed moderately better than a non-stochastic hill-climber. The hill-climber optimization technique essentially starts with an arbitrarily random possible solution to a problem (in this case, randomized weights for the neural network), then determines its fitness, that is, how well it performed, then incrementally changes the solution and compares the new changed solution's fitness to its parent. This project calculates a fitness based on whether or not the moonlander successfully landed, and if not, how far the lander was from conditions necessary to be considered safe. Specifically, the fitness is the sum of the vertical landing speed divided by the safe landing speed, and the horizontal position divided by the maximum safe distance from center. If it did land, it was considered fit. To improve the trainer's performance, the fitness calculation needs to be manually adjusted. The stochastic hill-climber algorithm differs from the non-stochastic in that it will not necessarily choose the steepest uphill move, but instead a random move from among the uphill possibilities, where the probability of the choice varies depending on the steepness.
\subsection{Problems}
Although a neural network can work as a controller for the moonlander, the biggest problem with it is that the trainer needs to run a full simulation for each attempt, and these simulations are somewhat computationally costly. Practically speaking, I could train across 10,000 episodes in about 10 minutes. Longer training sessions renders adjusting the scales and fitness calculations impractical, but this relatively low number of episodes means the testing was possibly reflecting the affects of each change I made incorrectly. Much better success could be achieved with a faster language and more computation time, as the hill-climber algorithm trains slowly (since it can only incrementally change one of many weights at a time) due to simulation time being slow.
\section{Results}
\section{Conclusion}
\end{document}
